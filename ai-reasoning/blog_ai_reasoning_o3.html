<html lang="en-US">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<title>OpenAI o3 model vs o1 model reasoning ability comparison and My Prediction</title>
<meta name="description" content="I will introduce you some of my analysis on the OpenAI's reasoning model o3 capabilities improvement over OpenAI o1 models. In Dec 2024 OpenAI's product and model releases, the last day's model release o3 is the most attractive. And I am wondering what's the difference between o1 and o3 model and what's are implication of the research path for the future. And here is what I have in mind">
<meta name="keywords" content="OpenAI,o3 model,reasoning">

<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/AI-Agents-for-Healthcare/favicon.ico" -->

<!-- end custom head snippets -->

</head>

<body>

<article class="markdown-body entry-content container-lg" itemprop="text"><div class="markdown-heading" dir="auto"><h1 tabindex="-1" class="heading-element" dir="auto">OpenAI o3 model vs o1 model reasoning ability comparison and My Prediction</h1><a id="user-content-openai-o3-model-vs-o1-model-reasoning-ability-comparison-and-my-prediction" class="anchor" aria-label="Permalink: OpenAI o3 model vs o1 model reasoning ability comparison and My Prediction" href="#openai-o3-model-vs-o1-model-reasoning-ability-comparison-and-my-prediction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Hi, in this blog, I will introduce you some of my analysis on the OpenAI's reasoning model o3 capabilities improvement over OpenAI o1 models.
In Dec 2024 OpenAI's product and model releases, the last day's model release o3 is the most attractive. And I am wondering what's the difference between o1 and o3 model
and what's are implication of the research path for the future. And here is what I have in mind:</p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto">1. OpenAI O3 Reasoning as Short-Cut Learning</h2><a id="user-content-1-openai-o3-reasoning-as-short-cut-learning" class="anchor" aria-label="Permalink: 1. OpenAI O3 Reasoning as Short-Cut Learning" href="#1-openai-o3-reasoning-as-short-cut-learning"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If we visualize the reasoning sampling process as a tree:</p>
<p dir="auto">On the left side is the shortcut learning (Short-Cut Learning) we pursued in the past: reaching the correct result with the fewest steps. On the right side is the 'reflection and backtracking' paradigm represented by OpenAI O1.</p>
<p dir="auto">We know that during the search process in O1, the model constantly reflects and backtracks, and this process often comes with additional costs. The question is, if the model can give the correct answer in one go, who would still want to spend time and money on complex searches? OpenAI isn’t foolish; everyone knows shortcuts are better!</p>
<p dir="auto">For more difficult problems, the potential thought tree becomes wider, and the search space at each step grows larger. The probability of reaching the correct answer via a shortcut becomes smaller. So, what can be done? An intuitive approach is to prune! Cut off the tree nodes that are unlikely to reach the endpoint in advance, compressing the search space—narrowing the tree back down. This is also the direction of many current efforts, for example:</p>
<p dir="auto">Chain of Preference Optimization naturally constructs preference data from the thought tree and uses DPO to optimize it, increasing the likelihood that the model will select tree nodes that can reach the endpoint. Outcome-supervised Value Models treat reasoning as an MDP process, using the probability (value) of reaching the correct answer at each step to guide strategy optimization.</p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto">2. Tree Search vs CoT Search</h2><a id="user-content-2-tree-search-vs-cot-search" class="anchor" aria-label="Permalink: 2. Tree Search vs CoT Search" href="#2-tree-search-vs-cot-search"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Back to O1, why choose to break away from the traditional shortcut approach and go down the 'detour' of Tree Search?</p>
<p dir="auto">In the past, we tended to exploit the basic capabilities of the model, and we believed that the existing GPT-4 model could already meet most conversational and simple reasoning needs. These tasks could be effectively sampled, preference-graded, and iteratively optimized.</p>
<p dir="auto">However, this perspective overlooks the needs of more complex tasks—such as mathematical reasoning (AIME, Frontier Math), code generation (SWE-Bench, CodeForce), and others. These tasks are often difficult to yield returns in the short term—they have very sparse rewards, only revealing rewards when the correct answer is eventually reached.</p>
<p dir="auto">Therefore, the traditional shortcut learning approach is no longer suitable for handling such complex tasks: if you can’t even sample a correct path, how can you optimize the model to select the right path?</p>
<p dir="auto">Returning to the 'Monte Carlo mindset' in the title of this article, we can see that this is essentially the same thing: the application of the Monte Carlo method in reinforcement learning is based on repeatedly sampling to estimate the value of a strategy, thereby optimizing the model. However, this method has inherent limitations—if the sampled strategies cannot find the optimal path, the model’s optimization will always end up with only a local optimum. This is why we choose more exploratory strategies in MC Learning.</p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto">3. Reasoning Capability improvement from o1 model to o3 model</h2><a id="user-content-3-reasoning-capability-improvement-from-o1-model-to-o3-model" class="anchor" aria-label="Permalink: 3. Reasoning Capability improvement from o1 model to o3 model" href="#3-reasoning-capability-improvement-from-o1-model-to-o3-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Before the release of O3, some people could imagine that the reasoning tokens of the O-series models would continue to increase. Following the previous pattern, there was an expectation that the increase would follow an empirical formula, improving by a certain multiple every few months. However, the information about O3 truly shocked everyone. In just 3 months, the inference cost increased by 2-3 orders of magnitude.</p>
<p dir="auto">In light of this result, I admit I had previously underestimated the speed of progress in the software field. Unlike chips, which are limited by manufacturing processes, physical conditions, and other factors, software can quickly consume all available hardware resources.</p>
<p dir="auto">Experienced engineers know that even for software solutions, when facing an increase in scale by several orders of magnitude, it’s not that easy. Often, each time the scale triples, new problems arise. O3’s ability to increase inference scale by more than 2 orders of magnitude so quickly is certainly not due to OpenAI’s exceptional team solving a lot of problems one after another in multiples of three.</p>
<p dir="auto">There are very few solutions that can quickly scale inference like this. The most typical ones involve various exploratory algorithms, where brute-force exploration can consume a lot of computation power, but the implementation is not that complicated. From simple breadth-first search to Monte Carlo Tree Search (MCTS), they all exhibit this characteristic.</p>
<p dir="auto">It’s reasonable to believe that O3 implements multi-path reasoning. Conversely, I don’t think O3 could extend the reasoning length by more than 2 orders of magnitude with single-path reasoning while maintaining consistent performance.</p>
<p dir="auto">When o1-preview was first released, I wrote a technical analysis of the O1 model, pointing out that the O1 series likely uses single-path reasoning. To this day, I still believe that’s the case. However, I currently don’t have a confident guess about the reasoning_effort parameter.</p>
<p dir="auto">But o1 Pro mode seems different. Its CoT summarization performance doesn’t appear to be the same as O1. My current observations aren’t sufficient yet to confidently make a guess based on its performance. I hope the O1 Pro mode API will be released soon.</p>
<p dir="auto">As for o3, it seems to use some form of multi-path reasoning, although it’s unclear exactly how these multiple paths are generated.</p>
<p dir="auto">Currently, there are two unexplained aspects of o1: the o1 model API cannot control the temperature, and the reasoning tokens of O1 are always multiples of 64."</p>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto">References</h2><a id="user-content-references" class="anchor" aria-label="Permalink: References" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div class="markdown-heading" dir="auto"><h4 tabindex="-1" class="heading-element" dir="auto">AI Agent Marketplace and Search</h4><a id="user-content-ai-agent-marketplace-and-search" class="anchor" aria-label="Permalink: AI Agent Marketplace and Search" href="#ai-agent-marketplace-and-search"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="http://www.deepnlp.org/search/agent" rel="nofollow">AI Agent Marketplace and Search</a> <br>
<a href="http://www.deepnlp.org/search/robot" rel="nofollow">Robot Search</a> <br>
<a href="http://www.deepnlp.org/search/equation" rel="nofollow">Equation and Academic search</a> <br>
<a href="http://www.deepnlp.org/search" rel="nofollow">AI &amp; Robot Comprehensive Search</a> <br>
<a href="http://www.deepnlp.org/question" rel="nofollow">AI &amp; Robot Question</a> <br>
<a href="http://www.deepnlp.org/community" rel="nofollow">AI &amp; Robot Community</a> <br></p>
<div class="markdown-heading" dir="auto"><h5 tabindex="-1" class="heading-element" dir="auto">AI Agent</h5><a id="user-content-ai-agent" class="anchor" aria-label="Permalink: AI Agent" href="#ai-agent"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="http://www.deepnlp.org/store/ai-agent" rel="nofollow">AI Agent Reviews</a> <br>
<a href="http://www.deepnlp.org/store/pub?category=ai-agent" rel="nofollow">AI Agent Publisher</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-microsoft-ai-agent" rel="nofollow">Microsoft AI Agents Reviews</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-claude-ai-agent" rel="nofollow">Claude AI Agents Reviews</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-openai-ai-agent" rel="nofollow">OpenAI AI Agents Reviews</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-agentgpt" rel="nofollow">AgentGPT AI Agents Reviews</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-salesforce-ai-agent" rel="nofollow">Saleforce AI Agents Reviews</a> <br>
<a href="http://www.deepnlp.org/store/ai-agent/ai-agent-builder" rel="nofollow">AI Agent Builder Reviews</a> <br></p>
<div class="markdown-heading" dir="auto"><h5 tabindex="-1" class="heading-element" dir="auto">AI Reasoning Chatbot</h5><a id="user-content-ai-reasoning-chatbot" class="anchor" aria-label="Permalink: AI Reasoning Chatbot" href="#ai-reasoning-chatbot"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="http://www.deepnlp.org/store/pub/pub-openai-o1" rel="nofollow">OpenAI o1 Reviews</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-openai-o3" rel="nofollow">OpenAI o3 Reviews</a> <br></p>
<div class="markdown-heading" dir="auto"><h5 tabindex="-1" class="heading-element" dir="auto">AI Video Generation</h5><a id="user-content-ai-video-generation" class="anchor" aria-label="Permalink: AI Video Generation" href="#ai-video-generation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="http://www.deepnlp.org/store/pub/pub-sora" rel="nofollow">Sora Reviews</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-kling-kwai" rel="nofollow">Kling AI Reviews</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-dreamina-douyin" rel="nofollow">Dreamina AI Reviews</a> <br>
<a href="http://www.deepnlp.org/store/pub" rel="nofollow">Best AI Apps Review</a> <br>
<a href="http://www.deepnlp.org/store/video-generator" rel="nofollow">AI Video Generator</a> <br>
<a href="http://www.deepnlp.org/store/image-generator" rel="nofollow">AI Image Generator</a> <br>
<a href="http://www.deepnlp.org/store/ai-glasses" rel="nofollow">AI Glasses Review</a> <br>
<a href="http://www.deepnlp.org/store/vr-glasses" rel="nofollow">VR Glasses Review</a> <br></p>
<div class="markdown-heading" dir="auto"><h5 tabindex="-1" class="heading-element" dir="auto">Robotics</h5><a id="user-content-robotics" class="anchor" aria-label="Permalink: Robotics" href="#robotics"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="http://www.deepnlp.org/store/pub/pub-tesla-cybercab" rel="nofollow">Tesla Cybercab Robotaxi</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-tesla-optimus" rel="nofollow">Tesla Optimus</a> <br>
<a href="http://www.deepnlp.org/store/pub/pub-figure-ai" rel="nofollow">Figure AI</a> <br></p>
</article>

</body></html>
